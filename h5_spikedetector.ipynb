{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H5 Spike Detector for Scaled and Filtered Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This version assumes you have pre-processed the data in h5_explorer\n",
    "- Reads in a h5 file of scaled and filtered experimental data from the Leaf Labs Willow System (h5_explorer.ipynb)\n",
    "- Smooths the time series and detects spikes above threshold (set by median +/- range of probe noise)\n",
    "- Plots the average trace for each channel according to probe geometry\n",
    "- Finds the width, peak amplitude, peak time, and inter-peak interval for each event\n",
    "- Saves the raw and scaled data to a csv file for analysis in any analysis program (SigmaPlot, Origin, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes this python notebook, the data file, and the imp. file are in same dir\n",
    "datafile = 'experiment_C20200330-174152_filtered_192.h5'\n",
    "shank_num = 1 # starting at 1\n",
    "fs = 30000 # sample rate\n",
    "window = [0, 30] # time window of analysis in seconds (for filtering, 4 mins max for most computers)\n",
    "columns = [[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21], \n",
    "            [22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41], \n",
    "            [42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63]]         \n",
    "lowcut = 400 # Hz for bandpass filter\n",
    "highcut = 5000 # Hz for bandpass filter\n",
    "order = 6 # For bandpass filter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from scipy.signal import butter, sosfiltfilt, sosfreqz\n",
    "import time\n",
    "import pandas as pd\n",
    "import scipy \n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the Multichannel Data\n",
    "- for a 5 minute file it takes 10-30 mins\n",
    "- keep checking for \"dead kernel\" errors\n",
    " - no dead kernel, it's still working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = np.array(columns)\n",
    "f_data = h5py.File(datafile, 'r')\n",
    "xs = f_data.get('sample_index') # channel # (x)\n",
    "ys = f_data.get('channel_data')  # actual data in microvolts (y)\n",
    "xs = xs[window[0]*fs:window[1]*fs] # Keep only the datapoints within our time window\n",
    "ys = ys[window[0]*fs:window[1]*fs, :] # Keep only the datapoints within our time window\n",
    "display (HTML(\"<hr><h4>Data consists of \" + str(ys.shape[1]) + \n",
    "              \" columns of data (channels, x) and \" + str(ys.shape[0]) + \n",
    "              \" rows of data (measurements, y).\" + \"The recording is \" +\n",
    "              str(round(ys.shape[0]/(fs * 60),3)) + \" mins long. <hr>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Savitzky-Golay Smoothing (Better than stringent band-pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgolay2d ( z, window_size, order, derivative=None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # number of terms in the polynomial expression\n",
    "    n_terms = ( order + 1 ) * ( order + 2)  / 2.0\n",
    "\n",
    "    if  window_size % 2 == 0:\n",
    "        raise ValueError('window_size must be odd')\n",
    "\n",
    "    if window_size**2 < n_terms:\n",
    "        raise ValueError('order is too high for the window size')\n",
    "\n",
    "    half_size = window_size // 2\n",
    "\n",
    "    # exponents of the polynomial. \n",
    "    # p(x,y) = a0 + a1*x + a2*y + a3*x^2 + a4*y^2 + a5*x*y + ... \n",
    "    # this line gives a list of two item tuple. Each tuple contains \n",
    "    # the exponents of the k-th term. First element of tuple is for x\n",
    "    # second element for y.\n",
    "    # Ex. exps = [(0,0), (1,0), (0,1), (2,0), (1,1), (0,2), ...]\n",
    "    exps = [ (k-n, n) for k in range(order+1) for n in range(k+1) ]\n",
    "\n",
    "    # coordinates of points\n",
    "    ind = np.arange(-half_size, half_size+1, dtype=np.float64)\n",
    "    dx = np.repeat( ind, window_size )\n",
    "    dy = np.tile( ind, [window_size, 1]).reshape(window_size**2, )\n",
    "\n",
    "    # build matrix of system of equation\n",
    "    A = np.empty( (window_size**2, len(exps)) )\n",
    "    for i, exp in enumerate( exps ):\n",
    "        A[:,i] = (dx**exp[0]) * (dy**exp[1])\n",
    "\n",
    "    # pad input array with appropriate values at the four borders\n",
    "    new_shape = z.shape[0] + 2*half_size, z.shape[1] + 2*half_size\n",
    "    Z = np.zeros( (new_shape) )\n",
    "    # top band\n",
    "    band = z[0, :]\n",
    "    Z[:half_size, half_size:-half_size] =  band -  np.abs( np.flipud( z[1:half_size+1, :] ) - band )\n",
    "    # bottom band\n",
    "    band = z[-1, :]\n",
    "    Z[-half_size:, half_size:-half_size] = band  + np.abs( np.flipud( z[-half_size-1:-1, :] )  -band )\n",
    "    # left band\n",
    "    band = np.tile( z[:,0].reshape(-1,1), [1,half_size])\n",
    "    Z[half_size:-half_size, :half_size] = band - np.abs( np.fliplr( z[:, 1:half_size+1] ) - band )\n",
    "    # right band\n",
    "    band = np.tile( z[:,-1].reshape(-1,1), [1,half_size] )\n",
    "    Z[half_size:-half_size, -half_size:] =  band + np.abs( np.fliplr( z[:, -half_size-1:-1] ) - band )\n",
    "    # central band\n",
    "    Z[half_size:-half_size, half_size:-half_size] = z\n",
    "\n",
    "    # top left corner\n",
    "    band = z[0,0]\n",
    "    Z[:half_size,:half_size] = band - np.abs( np.flipud(np.fliplr(z[1:half_size+1,1:half_size+1]) ) - band )\n",
    "    # bottom right corner\n",
    "    band = z[-1,-1]\n",
    "    Z[-half_size:,-half_size:] = band + np.abs( np.flipud(np.fliplr(z[-half_size-1:-1,-half_size-1:-1]) ) - band )\n",
    "\n",
    "    # top right corner\n",
    "    band = Z[half_size,-half_size:]\n",
    "    Z[:half_size,-half_size:] = band - np.abs( np.flipud(Z[half_size+1:2*half_size+1,-half_size:]) - band )\n",
    "    # bottom left corner\n",
    "    band = Z[-half_size:,half_size].reshape(-1,1)\n",
    "    Z[-half_size:,:half_size] = band - np.abs( np.fliplr(Z[-half_size:, half_size+1:2*half_size+1]) - band )\n",
    "\n",
    "    # solve system and convolve\n",
    "    if derivative == None:\n",
    "        m = np.linalg.pinv(A)[0].reshape((window_size, -1))\n",
    "        return scipy.signal.fftconvolve(Z, m, mode='valid')\n",
    "    elif derivative == 'col':\n",
    "        c = np.linalg.pinv(A)[1].reshape((window_size, -1))\n",
    "        return scipy.signal.fftconvolve(Z, -c, mode='valid')\n",
    "    elif derivative == 'row':\n",
    "        r = np.linalg.pinv(A)[2].reshape((window_size, -1))\n",
    "        return scipy.signal.fftconvolve(Z, -r, mode='valid')\n",
    "    elif derivative == 'both':\n",
    "        c = np.linalg.pinv(A)[1].reshape((window_size, -1))\n",
    "        r = np.linalg.pinv(A)[2].reshape((window_size, -1))\n",
    "        return scipy.signal.fftconvolve(Z, -r, mode='valid'), scipy.signal.fftconvolve(Z, -c, mode='valid')\n",
    "    \n",
    "smoothed = sgolay2d( ys, window_size=41, order=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot a sample of the smoothed data\n",
    "- If the scripts are running too slow, skip the boxes with figures/plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0   # Start time of sample (in seconds)\n",
    "end = 1000  # End time of sample (in seconds)\n",
    "c = 4       # Channel to analyze "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (20,4))\n",
    "ax.plot(xs[start:end]/fs,ys[start:end,c])\n",
    "ax.plot(xs[start:end]/fs,smoothed[start:end,c])\n",
    "plt.savefig (datafile.replace('.h5','_smoothed_chan' + str(c) + '.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for events with 1 threshold for all channels\n",
    "- Search in 5s windows, get mean of smoothed data and sd\n",
    "- from start to end of window find any peaks that are greater than 1 sd of baseline or compare MSE?\n",
    "- save all data points from -3 to + 3 ms around peak and note the time of the peak\n",
    "    - later look for overlap between epochs to determine wavelets vs single spikes \n",
    "    - i.e. granule cell activity from simple spikes\n",
    "- Experimented with dataframes, np.arays, and dicts for working with these long lists of events and dataframes were the worst for speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_search = 0    # Time to start looking for events (in seconds)\n",
    "end_search = 1      # Time to stop looking for events (in seconds)\n",
    "threshold = 0.5       # Multiplier of the std of the smoothed mean for every 10ms with 10% overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_search(raw, smooth, start, end, thresh): \n",
    "    start = int(start * fs)\n",
    "    end = int(end*fs)\n",
    "    mn = fs-150 # minimum length of analysis window (0.995 s)\n",
    "    peaks_x = {} # dict to catch peak times \n",
    "    peaks_y = {} # dict to catch peak amps\n",
    "    peaks_b = {} # dict to catch peak baseline amps\n",
    "    events_x = {} # dict to catch event trace x (3 ms before and after peak)\n",
    "    events_y = {} # dict to catch event trace y (3 ms before and after peak)\n",
    "    events_b = {} # dict to catch event trace baseline (3 ms before and after peak)\n",
    "    print('Analyzing channel: ', end = ' ')\n",
    "    for j in np.arange(0,64):\n",
    "        print(j, end = ' ')\n",
    "        i = start\n",
    "        epoch = 0\n",
    "        peak_x = []\n",
    "        peak_y = []\n",
    "        peak_b = []\n",
    "        rep = 0\n",
    "        while i < end-11: # search all the rows \n",
    "            if i % mn == 0 and i < end - mn: # moving window every 10ms\n",
    "                savg = np.median(abs(smooth[i:i+mn,j]))\n",
    "                ssd = np.std(smooth[i:i+mn,j])*threshold\n",
    "            if raw[i,j] > savg *ssd  and raw[i,j] > max(raw[i+1:i+10,j]) and raw[i,j] > raw[i-1,j] and i-10 >= 0:\n",
    "                peak_x.append(i)\n",
    "                peak_y.append(raw[i,j])\n",
    "                peak_b.append(savg)\n",
    "                events_x['xCh'+str(j)+'_'+str(rep)] = list(range(i-10,i+10))\n",
    "                events_y['yCh'+str(j)+'_'+str(rep)] = raw[i-10:i+10,j]\n",
    "                rep = rep + 1\n",
    "            i = i + 1\n",
    "        peaks_x[str(j)] = peak_x\n",
    "        peaks_y[str(j)] = peak_y\n",
    "        peaks_b[str(j)] = peak_b\n",
    "    peaks = [peaks_x,peaks_y,peaks_b]\n",
    "    events = [events_x, events_y]\n",
    "    return peaks, events\n",
    "            \n",
    "peaks, events = event_search (ys, smoothed, start_search, end_search, threshold)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Search for events with a different threshold for each channel\n",
    "- same as with 1 threshold for all, just adjust the threshold variable to be an array with the same dimensions as the `columns` variable in the User Input section at the top of the notebook.\n",
    "- Note that there is one set of brackets, not two for thresholds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_search = 0    # Time to start looking for events (in seconds)\n",
    "end_search = 1      # Time to stop looking for events (in seconds)\n",
    "thresholds = [0.5, 0.5, 0.5, 0.5, 0.5, 1, 1, 0.5, 0.1, 0.2, 2, 0.5, 0.4, 1, 1, 1, 0.3, 1, 2, 1, 0.2, 0.5, \n",
    "              0.5, 0.5, 0.5, 0.5, 0.5, 1, 1, 0.5, 0.1, 0.2, 2, 0.5, 0.4, 1, 1, 1, 0.3, 1, 2, 1, \n",
    "              0.5, 0.5, 0.5, 0.5, 0.5, 1, 1, 0.5, 0.1, 0.2, 0.01, 0.5, 0.4, 0.2, 0.2, 0.2, 0.3, 1, 2, 1, 0.2, 0.5]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_search(raw, smooth, start, end, thresh): \n",
    "    start = int(start * fs)\n",
    "    end = int(end*fs)\n",
    "    mn = fs-150 # minimum length of analysis window (0.995 s)\n",
    "    peaks_x = {} # dict to catch peak times \n",
    "    peaks_y = {} # dict to catch peak amps\n",
    "    peaks_b = {} # dict to catch peak baseline amps\n",
    "    events_x = {} # dict to catch event trace x (3 ms before and after peak)\n",
    "    events_y = {} # dict to catch event trace y (3 ms before and after peak)\n",
    "    events_b = {} # dict to catch event trace baseline (3 ms before and after peak)\n",
    "    print('Analyzing channel: ', end = ' ')\n",
    "    for j in np.arange(0,64):\n",
    "        print(j, end = ' ')\n",
    "        i = start\n",
    "        epoch = 0\n",
    "        peak_x = []\n",
    "        peak_y = []\n",
    "        peak_b = []\n",
    "        rep = 0\n",
    "        while i < end-11: # search all the rows \n",
    "            if i % mn == 0 and i < end - mn: # moving window every 10ms\n",
    "                savg = np.median(abs(smooth[i:i+mn,j]))\n",
    "                ssd = np.std(smooth[i:i+mn,j])*thresh[j]\n",
    "            if raw[i,j] > savg *ssd  and raw[i,j] > max(raw[i+1:i+10,j]) and raw[i,j] > raw[i-1,j] and i-10 >= 0:\n",
    "                peak_x.append(i)\n",
    "                peak_y.append(raw[i,j])\n",
    "                peak_b.append(savg)\n",
    "                events_x['xCh'+str(j)+'_'+str(rep)] = list(range(i-10,i+10))\n",
    "                events_y['yCh'+str(j)+'_'+str(rep)] = raw[i-10:i+10,j]\n",
    "                rep = rep + 1\n",
    "            i = i + 1\n",
    "        peaks_x[str(j)] = peak_x\n",
    "        peaks_y[str(j)] = peak_y\n",
    "        peaks_b[str(j)] = peak_b\n",
    "    peaks = [peaks_x,peaks_y,peaks_b]\n",
    "    events = [events_x, events_y]\n",
    "    return peaks, events\n",
    "\n",
    "thresholds = np.array(thresholds)\n",
    "peaks, events = event_search (ys, smoothed, start_search, end_search, thresholds)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot a subset of the events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = int(0 * fs) # Start of window to see how well search performed   \n",
    "end = int(29.5* fs) # End of window to see how well search performed\n",
    "chans = list(np.arange(0,64,4)) # Range of channels to preview (start, end, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for chan in chans:\n",
    "    fig, ax = plt.subplots(figsize = (20,4))\n",
    "    ax.plot(xs[start:end],ys[start:end,chan])\n",
    "    ax.plot(xs[start:end],smoothed[start:end,chan])\n",
    "    ax.plot(peaks[0][str(chan)],\n",
    "        peaks[1][str(chan)], \n",
    "        ls = 'none', marker = '*', markersize = 6, color = 'black')\n",
    "    #ax.axhline (peaks[2][str(chan)][50], ls = ':')\n",
    "    ax.set_title(\"Channel \" + str(chan), fontsize = 20)\n",
    "    xticks = ax.get_xticks()\n",
    "    xticks = [int(t/30000) for t in xticks]\n",
    "    ax.set_xticklabels (xticks, fontsize = 14)\n",
    "    ax.set_ylabel(r'$\\mu$V', fontsize = 18)\n",
    "    ax.set_xlabel('Time (s)', fontsize = 18)\n",
    "    ax.set_xlim(0.2 *fs,.3*fs)\n",
    "    #ax.set_ylim(-100,200)\n",
    "plt.tight_layout()\n",
    "plt.savefig (datafile.replace('.h5','_scatterpeaks.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Events Dictionaries to Dataframes for easy file handling\n",
    "- These can be used to reconstruct raw traces of individual events in other programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "idx,id_len = get_index_from_dict(events[0])\n",
    "df_events_x = pd.DataFrame(events[0], columns = events[0].keys(), index = idx)\n",
    "df_events_y = pd.DataFrame(events[1], columns = events[1].keys(), index = idx)\n",
    "df_events = df_events_x.join(df_events_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the events and peaks to csv files\n",
    "- These may get too big to open in excel\n",
    "- If so, break them down into 64 channels each with 2 min recordings\n",
    "- Use this as a checkpoint. If your kernel crashes, you can recreate the dicts quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events.to_csv(datafile.replace('.h5', '_event_traces.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the mean traces for each event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "widths = {} # Dict for width\n",
    "ipis = {} # Dict for Iinter-peak interval\n",
    "x = list(range(0,20))\n",
    "y_means = []\n",
    "print(\"Averaging channel: \", end = ' ')\n",
    "for chan in np.arange(0,64):\n",
    "    print(str(chan), end = ' ')\n",
    "    avg_cols = pd.DataFrame()\n",
    "    x_width = []\n",
    "    ipi = []\n",
    "    for key in events[1].keys():\n",
    "        if 'yCh'+str(chan) in key:\n",
    "            # Get y values to average\n",
    "            avg_cols[key] = events[1][key] \n",
    "            \n",
    "            # Get interevent interval\n",
    "            ipi_list = peaks[0][str(chan)]\n",
    "            for i in np.arange(len(ipi_list)-1):\n",
    "                ipi.append(ipi_list[i+1]-ipi_list[i])\n",
    "            # Get width of peak\n",
    "            tmp1 = 0\n",
    "            tmp2 = 0\n",
    "            events_list = events[1][key]\n",
    "            for x in np.arange(10,0,-1):\n",
    "                if events_list[x] < events_list[(x-1)]:\n",
    "                    tmp1 = x  \n",
    "            for x in np.arange(11,19):\n",
    "                if events_list[x] > events_list[(x+1)]:\n",
    "                    tmp2 = x\n",
    "            x_width.append(tmp2-tmp1)\n",
    "        \n",
    "        widths[str(chan)] = x_width\n",
    "        ipis[str(chan)] = ipi\n",
    "    y_mean = avg_cols.transpose().mean()\n",
    "    y_means.append(y_mean)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def grid_plot_events(x,y):\n",
    "    chan_list = []\n",
    "    display(HTML('<hr><h4>Plotting channel: '))\n",
    "    fig, ax = plt.subplots (len(columns[0]),int(columns.shape[0]), figsize =(15,40), sharex = True, sharey = True)\n",
    "    x = [i/(fs) for i in x]\n",
    "    j = 0\n",
    "    for j in np.arange (0,columns.shape[0]):\n",
    "        for i,col in enumerate(columns[j]): \n",
    "            print(col, end = ' ')\n",
    "            if j == 1: # Offset for middle row (grid is 66 panels, but we only have 64 channels. Middle row is shorter) \n",
    "                i = i + 1\n",
    "            if np.isnan(y_means[col]).all():\n",
    "                continue\n",
    "            else:\n",
    "                chan_list.append(col)\n",
    "            ax[i][j].plot(x, y_means[col], color = 'dimgray', \n",
    "                            label=str(col)) # Filtered Signal\n",
    "            handles, labels = ax[i][j].get_legend_handles_labels()\n",
    "            ax[i][j].legend(handles, labels, loc = 'upper right', fontsize = 8, shadow = False)\n",
    "            plt.tight_layout()\n",
    "    fig.text(0.0, 0.5, r'Amplitude ($\\mu$V)', ha='center', rotation='vertical', fontsize = 18)\n",
    "    fig.text(0.5, 0.0, 'Time (s)', va='center',  fontsize = 18)\n",
    "    plt.savefig (datafile.replace('.h5','.png'))\n",
    "    display(HTML('<hr>'))\n",
    "\n",
    "x = list(range(len(events[0]['xCh0_0'])))\n",
    "grid_plot_events(x, y_means)\n",
    "plt.savefig (datafile.replace('.h5','_peakgrid.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format final output\n",
    "- takes a long time. DFs will add processing time, so if you have to pull data, do it from the peaks and events dictionaries, not the dataframe\n",
    "- ids are 'Ch + channel# + event#'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f_out = open(datafile.replace('.h5','_event_data.csv'), \"w\")\n",
    "writer = csv.writer(f_out)\n",
    "header = ['id','time_datapoints','time_seconds','peak_amplitude_microvolts','peak_width_datapoints',\n",
    "          'peak_width_seconds','interstimulus_interval_datapoints', 'interpeak_interval_seconds',\n",
    "          'baseline_amplitude_microvolts']\n",
    "writer.writerow(header)\n",
    "print('Working on chan: ', end = ' ')\n",
    "for chan in np.arange(64):\n",
    "    # load the data for the channel upfront for speed\n",
    "    str_chan = str(chan)\n",
    "    row_range = range(len(peaks[0][str(chan)]))\n",
    "    w_list = widths[str_chan]\n",
    "    x_list = peaks[0][str_chan]\n",
    "    y_list = peaks[1][str_chan]\n",
    "    b_list = peaks[2][str_chan]\n",
    "    n_list = ipis[str_chan]\n",
    "    print(chan, end = ' ')\n",
    "    for i in row_range:\n",
    "        x_dp = x_list[i]\n",
    "        y = y_list[i]\n",
    "        b = b_list[i]\n",
    "        if i == 0:\n",
    "            w = w_list[i]\n",
    "            n = np.nan\n",
    "            row = ['Ch'+str(chan)+str(x_dp),x_dp,(x_dp/fs),y, w, (w/fs), n, (n/fs), b]\n",
    "            writer.writerow(row)\n",
    "        elif i!=0 and i < len(widths[str(chan)]):\n",
    "            w = w_list[i]\n",
    "            n = n_list[i]\n",
    "            row = ['Ch'+str(chan)+str(x_dp),x_dp,(x_dp/fs),y, w, (w/fs), n, (n/fs), b]\n",
    "            writer.writerow(row)\n",
    "f_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Output to Nex \n",
    "- column values are Channel number in the format (Ch1)\n",
    "- row values are peak times in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = {}\n",
    "max_rows = max([len(peaks[0][str(chan)]) for chan in np.arange(64)])\n",
    "print('Working on chan: ', end = ' ')\n",
    "for chan in np.arange(64):\n",
    "    str_chan = str(chan)\n",
    "    cols['Ch'+str(chan)] = [peak/fs for peak in peaks[0][str_chan]]\n",
    "    cols['Ch'+str(chan)].extend(np.nan for t in np.arange(len(peaks[0][str_chan]),max_rows)) # Fill in missing values with NaN\n",
    "    \n",
    "    print(chan, end = ' ')\n",
    "df = pd.DataFrame(cols, index = list(np.arange(max_rows)))\n",
    "df = df.to_csv(datafile.replace('.h5','_events_nex.csv'), index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(max_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "willow",
   "language": "python",
   "name": "willow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
